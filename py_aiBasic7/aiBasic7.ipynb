{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfobKjJRIKMC",
    "outputId": "99ddb865-e135-4e41-c537-a657e06e6d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.712997079\n",
      "Epoch: 0002 cost = 0.430425495\n",
      "Epoch: 0003 cost = 0.384661943\n",
      "Epoch: 0004 cost = 0.361041397\n",
      "Epoch: 0005 cost = 0.346561611\n",
      "Epoch: 0006 cost = 0.335881978\n",
      "Epoch: 0007 cost = 0.328014225\n",
      "Epoch: 0008 cost = 0.321718723\n",
      "Epoch: 0009 cost = 0.316227525\n",
      "Epoch: 0010 cost = 0.312065363\n",
      "Epoch: 0011 cost = 0.308147997\n",
      "Epoch: 0012 cost = 0.304742455\n",
      "Epoch: 0013 cost = 0.302093029\n",
      "Epoch: 0014 cost = 0.299490869\n",
      "Epoch: 0015 cost = 0.296950489\n",
      "Epoch: 0016 cost = 0.295024037\n",
      "Epoch: 0017 cost = 0.292878270\n",
      "Epoch: 0018 cost = 0.291426718\n",
      "Epoch: 0019 cost = 0.289959490\n",
      "Epoch: 0020 cost = 0.288146228\n",
      "Learning finished\n",
      "time : 148.33100175857544\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "# parameters\n",
    "training_epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "linear = torch.nn.Linear(784, 10, bias=True).to(device)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "start = time.time()  # start time\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "print(\"time :\", time.time() - start )  # Q1) Complete the code to print the total execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "RtnIw1LOMTAQ",
    "outputId": "21e67a96-c529-41d0-8443-e7eec7066d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8953999876976013\n",
      "Label:  9\n",
      "Prediction:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Q2) Complete the following code so that read and print one test data of the last 4 digits of your student ID.\n",
    "    # ex. In case of s_id = 2008710991, read and print 0992th test data sample.\n",
    "    r = 3504\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBhihZDe-Taj",
    "outputId": "5c056c5c-8698-45ae-b695-a6724d879e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "[Epoch:    1] cost = 0.15985404\n",
      "[Epoch:    2] cost = 0.0418346673\n",
      "[Epoch:    3] cost = 0.0296812151\n",
      "[Epoch:    4] cost = 0.0218928456\n",
      "[Epoch:    5] cost = 0.0177077837\n",
      "[Epoch:    6] cost = 0.0145505527\n",
      "[Epoch:    7] cost = 0.0122586144\n",
      "[Epoch:    8] cost = 0.0101227099\n",
      "[Epoch:    9] cost = 0.0104314396\n",
      "[Epoch:   10] cost = 0.00657672575\n",
      "[Epoch:   11] cost = 0.00864209235\n",
      "[Epoch:   12] cost = 0.00513090659\n",
      "[Epoch:   13] cost = 0.00802448392\n",
      "[Epoch:   14] cost = 0.00569790928\n",
      "[Epoch:   15] cost = 0.00606314372\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(0)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "# CNN Model (2 conv layers)\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
    "        #    Conv     -> (?, 28, 28, 32)\n",
    "        #    Pool     -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
    "        #    Conv      ->(?, 14, 14, 64)\n",
    "        #    Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # Final FC 7x7x64 inputs -> 10 outputs\n",
    "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # Final FC 7x7x64 inputs -> 10 outputs\n",
    "        self.fc1 = torch.nn.Linear(3 * 3 * 128, 625, bias=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(625, 10, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# instantiate CNN model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "total_batch = len(data_loader)\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        # image is already size of (28x28), no reshape\n",
    "        # label is not one-hot encoded\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2QYsweQ-VRf",
    "outputId": "80ba5835-c092-4423-c303-4a6015359348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9810000061988831\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ai_bu_04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
